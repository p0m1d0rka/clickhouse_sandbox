`vagrant up`
Поднимает кластер кликхауса из 2 шардов, в каждом из котором две реплики.

ip адреса машинок(можно поменять в vagrantfile, если будут конфликты):
`sh` - shard
`r` - replica
192.168.50.11 - sh1r1
192.168.50.12 - sh1r2
192.168.50.21 - sh2r1
192.168.50.22 - sh2r2


После чего можно подключаться под обычным юзером, например через dbeaver или любую другую тулзу. Порт 8123 маппится на хост.
user = `admin`
password = `12345`

Так же создается таблица `test.hits_local` и распределенная таблица `test.hits_all` 

Если необходимо загрузить в нее данные, то данные сперва необходимо скачать
`vagrant ssh`
`curl https://clickhouse-datasets.s3.yandex.net/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` > hits_v1.tsv`

Загружаем данные `clickhouse-client --query "INSERT INTO test.hits_all FORMAT TSV" --max_insert_block_size=100000 < hits_v1.tsv`
Если вышла ошибка о нехватке памяти, то можно либо увеличить количество памяти в vm, (параметр `vb.memory`) , либо загрузить часть данных.
Сырые данные занимают 7.3 GB, 1млн записей - около 800мб
1. отбираем 1 000 000 записей `head -n 1000000 > hits_v1_1000000_rows.tsv`
2. записываем из в таблицу `clickhouse-client --query "INSERT INTO test.hits_all FORMAT TSV" --max_insert_block_size=100000 < hits_v1_1000000_rows.tsv`

Данные будут распределены между двумя шардами sh1 и sh2. В этом можно убедиться, зайдя на каждый шард и посмотрев количество записей в таблицах `test.hits_local`


Целевой кластер - два шарда, две реплики в каждом шарде, зукипер

